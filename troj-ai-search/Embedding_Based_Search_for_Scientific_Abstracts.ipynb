{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding-based Search for Scientific Abstracts\n",
    "\n",
    "#### Alden Dima  \n",
    "alden.dima@nist.gov  \n",
    "Information Systems Group  \n",
    "Information Technology Laboratory  \n",
    "National Institute of Standards and Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terms of Use\n",
    "\n",
    "This software was developed at the [National Institute of Standards and Technology (NIST)](https://www.nist.gov) by employees of the Federal Government in the course of their official duties.  Pursuant to Title 17 Section 105 of the United States Code this software is not subject to copyright protection and is in the public domain.  It is an experimental system.  NIST assumes no responsibility whatsoever for its use by other parties, and makes no guarantees, expressed or implied, about its quality, reliability, or any other characteristic.  We would appreciate acknowledgement if the software is used.\n",
    "\n",
    "This software can be redistributed and/or modified freely provided that any derivative works bear some notice that they are derived from it, and any modified versions bear some notice that they have been modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "This Jupyter notebook contains a prototype embedding-based text search method developed as a part of NIST's participation in the IARPA TrojAI Project to help accelerate the manual summarization of TrojAI-related literature being curated at the [TrojAI Literature Review GitHub repository](https://github.com/usnistgov/trojai-literature). \n",
    "\n",
    "Our method uses an [flair](https://github.com/flairNLP/flair) embeddings and a [gensim](https://radimrehurek.com/gensim/) similarity index created from the computer science subset of [arXiv](https://arxiv.org/). We then produce a list of the top-ranked abstracts as the results of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import Sentence\n",
    "from flair.embeddings import FlairEmbeddings, DocumentPoolEmbeddings\n",
    "import torch\n",
    "import gensim.matutils\n",
    "from gensim.similarities import Similarity\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import pickle\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking previously created forwards and backwards flair embeddings for arXiv CS titles and abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_forward = FlairEmbeddings('language_model/arXiv-cs-lc-forward.pt')\n",
    "cs_backward = FlairEmbeddings('language_model/arXiv-cs-lc-backward.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = DocumentPoolEmbeddings(\n",
    "                        [\n",
    "                            cs_forward,\n",
    "                            cs_backward\n",
    "                        ],\n",
    "                        pooling='mean',\n",
    "                        fine_tune_mode='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract titles and text saved in Python pickle format from the embedding and index creation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('title-abstracts.pkl', 'rb') as fsent:\n",
    "    sents = pickle.load(fsent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading previously created gensim similiarity index containing embedding of each documents title and abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = Similarity.load('title-abstracts.idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some utility functions for creating queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the spaces makes a big difference with character-based embeddings!\n",
    "spat = re.compile(\"\\s+\")\n",
    "\n",
    "def normspaces(s):\n",
    "    return re.sub(spat, \" \", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(s):\n",
    "    sent = Sentence(normspaces(s))\n",
    "    document_embeddings.embed(sent)\n",
    "    emb = sent.get_embedding().cpu().detach().numpy()\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb2sparse(emb):\n",
    "    sparse = gensim.matutils.any2sparse(emb)\n",
    "    return sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are using a document's title and abstract as a query to create a ranked list of other similary documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"\n",
    "Neural Trojans. While neural networks demonstrate stronger capabilities in \n",
    "pattern recognition nowadays, they are also becoming larger and deeper. As a \n",
    "result, the effort needed to train a network also increases dramatically. In \n",
    "many cases, it is more practical to use a neural network intellectual property \n",
    "(IP) that an IP vendor has already trained. As we do not know about the training \n",
    "process, there can be security threats in the neural IP: the IP vendor \n",
    "(attacker) may embed hidden malicious functionality, i.e. neural Trojans, into \n",
    "the neural IP. We show that this is an effective attack and provide three \n",
    "mitigation techniques: input anomaly detection, re-training, and input \n",
    "preprocessing. All the techniques are proven effective. The input anomaly \n",
    "detection approach is able to detect 99.8% of Trojan triggers although with \n",
    "12.2% false positive. The re-training approach is able to prevent 94.1% of \n",
    "Trojan triggers from triggering the Trojan although it requires that the neural \n",
    "IP be reconfigurable. In the input preprocessing approach, 90.2% of Trojan \n",
    "triggers are rendered ineffective and no assumption about the neural IP is \n",
    "needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_selected = 50\n",
    "qry = qry.strip().lower()\n",
    "q = emb2sparse(embed_sentence(qry))\n",
    "hits = sorted(enumerate(idx[q]), key=lambda x:x[1], reverse=True)\n",
    "hits_selected = hits[0:num_selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check - we expect the maximum similarity to be close to one for documents that are already in the arXiv CS subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(idx[q])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a word cloud to visualize the top results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_selected = [sents[i] for i,v in hits_selected]\n",
    "text_selected = \" \".join(sents_selected)\n",
    "wordcloud = WordCloud(height=750, \n",
    "                      width=1000, \n",
    "                      random_state=43,\n",
    "                      background_color='white',\n",
    "                      color_func=lambda *args, **kwargs: \"black\",\n",
    "                      relative_scaling=1).generate(text_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The top ranked search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i,v in hits_selected:\n",
    "    c += 1\n",
    "    print(\"{}\\t{}\\t{}\\n\".format(c, format(v.item(), \".2f\"), sents[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
