{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarization of Scientific Articles\n",
    "\n",
    "#### Alden Dima\n",
    "alden.dima@nist.gov  \n",
    "Information Systems Group  \n",
    "Information Technology Laboratory  \n",
    "National Institute of Standards and Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "This Jupyter notebook contains a prototype extractive text summarization method developed as a part of NIST's participation in the IARPA TrojAI Project to help accelerate the manual summarization of TrojAI-related literature being curated at the [TrojAI Literature Review GitHub repository](https://github.com/usnistgov/trojai-literature). For each document, our method identifies sentences containing certain metadiscourse markers and then ranks them using this [implementation](https://pypi.org/project/lexrank/) of the [LexRank](https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html) algorithm. We then emit the top-ranked sentences as a summary of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terms of Use\n",
    "\n",
    "This software was developed at the [National Institute of Standards and Technology (NIST)](https://www.nist.gov) by employees of the Federal Government in the course of their official duties.  Pursuant to Title 17 Section 105 of the United States Code this software is not subject to copyright protection and is in the public domain.  It is an experimental system.  NIST assumes no responsibility whatsoever for its use by other parties, and makes no guarantees, expressed or implied, about its quality, reliability, or any other characteristic.  We would appreciate acknowledgement if the software is used.\n",
    "\n",
    "This software can be redistributed and/or modified freely provided that any derivative works bear some notice that they are derived from it, and any modified versions bear some notice that they have been modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Strategy\n",
    "\n",
    "We start with text extracted from PDF documents using [pdftotext](http://www.xpdfreader.com/). For each document's text, we:\n",
    "1. Segment the sentences and create a language model for the summarizer\n",
    "1. Identify the sentences which have metadiscource markers\n",
    "1. Apply LexRank to those sentences to rank them by their centrality\n",
    "1. Emit the top N sentences as a summary for that document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust location of and extension for document text files as needed\n",
    "TEXT_DIR = 'data/text/'\n",
    "TEXT_EXT = 'txt'\n",
    "\n",
    "# Metadiscourse marker sets - see MetaMarker class below\n",
    "MARKER_SETS = {0, 1}\n",
    "\n",
    "# LexRank parameters\n",
    "SUMMARY_SIZE = 7 # Number of sentences in summary\n",
    "THRESHOLD = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import regex as re\n",
    "import spacy\n",
    "import en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "from lexrank import STOPWORDS, LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dir = Path(TEXT_DIR)\n",
    "text_files = list(text_dir.glob(\"*.{}\".format(TEXT_EXT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements heuristics to identify sentences with metadiscourse markers.\n",
    "\n",
    "class MetaMarkers:\n",
    "    def __init__(self, pron = True, marker_sets = MARKER_SETS):\n",
    "        mw = [\n",
    "            # marker set 0\n",
    "            set(['paper', 'work', 'research', 'article', \n",
    "                 'study', 'publication', 'section', 'approach', \n",
    "                 'method', 'technique', 'results']),\n",
    "            \n",
    "            # marker set 1\n",
    "            set(['propose', 'present', 'exploit', 'investigate', \n",
    "                 'show', 'provide', 'explore',\n",
    "                 'focus', 'consider', 'implement', 'adopt', \n",
    "                 'examine', 'expand', 'prove', 'argue', \n",
    "                 'claim', 'suggest', 'contrast', 'summarize']),\n",
    "            \n",
    "            # marker set 2\n",
    "            set(['better', 'significant', 'first', 'second', \n",
    "                 'third', 'begin', 'finally', 'therefore', \n",
    "                 'however', 'consequently']),\n",
    "        ]\n",
    "        \n",
    "        self.marker_tags = set(['PRON'])\n",
    "        \n",
    "        self.marker_words = set()\n",
    "        try:\n",
    "            for m in marker_sets:\n",
    "                self.marker_words.update(mw[m])\n",
    "        except (IndexError, TypeError):\n",
    "            print(\"Invalid marker specifier value\", file=sys.stderr)\n",
    "            raise\n",
    "\n",
    "    def is_meta(self, sent):\n",
    "        pos = set([str(w.pos_) for w in sent])\n",
    "        tok = set([str(w.lemma_) for w in sent])\n",
    "        result = self.marker_tags.intersection(pos) or self.marker_words.intersection(tok)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segment all sentences of documents to be summarized and create a language model for the summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_pat = re.compile(\"\\s+\") # Used to normalize whitespace\n",
    "all_sents = {}             # Docs to be summarized, indexed by file name\n",
    "docs = []                  # Used to create lexRank's language model\n",
    "for text_file in text_files:\n",
    "    with open(text_file) as fin:\n",
    "        text = fin.read()\n",
    "        doc = nlp(text)\n",
    "        sents = [s for s in doc.sents]\n",
    "        all_sents[text_file] = sents\n",
    "        docs.append(str(sents))\n",
    "        \n",
    "lxr = LexRank(docs, stopwords=STOPWORDS['en']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify sentences with metadiscourse markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heur_sents = {} # Contains sentences with metadiscourse markers\n",
    "sent_order = {} # Maintains sentence appearance order\n",
    "\n",
    "mm = MetaMarkers()\n",
    "\n",
    "for f, sents in all_sents.items():\n",
    "    my_sents = []\n",
    "    for s in sents:\n",
    "        if mm.is_meta(s):\n",
    "            my_sents.append(str(s).strip())\n",
    "    heur_sents[f] = my_sents\n",
    "    sent_order[f] = {s:n for (n,s) in enumerate(my_sents)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: For how many of the documents do we have sentences with metadiscourse markers?\n",
    "\n",
    "num = len({f for (f,t) in heur_sents.items() if t != []})\n",
    "denom = len({f for (f,t) in heur_sents.items()})\n",
    "print(\"{} out of {} documents have sentences\".format(num, denom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply LexRank and emit top N sentences in order of appearance in original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Applying \"Classical LexRank\"\n",
    "\n",
    "for f,s in heur_sents.items():\n",
    "    summary = lxr.get_summary(s, summary_size=SUMMARY_SIZE, threshold=THRESHOLD)\n",
    "    sorted_sents = sorted([(sent_order[f][s],s) for s in summary])\n",
    "    \n",
    "    # Cleaning up embedded newlines and other whitespace issues with the sentences that we'll keep.\n",
    "    summary_sents = [re.sub(ws_pat, ' ', str(s)) for (_,s) in sorted_sents]\n",
    "    print(\"{}: {}\\n\".format(f.name, \" ... \".join(summary_sents))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
