{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarization of Scientific Articles\n",
    "\n",
    "#### Alden Dima\n",
    "alden.dima@nist.gov  \n",
    "Information Systems Group  \n",
    "Information Technology Laboratory  \n",
    "National Institute of Standards and Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "This Jupyter notebook contains a prototype extractive text summarization method developed as a part of NIST's participation in the IARPA TrojAI Project to help accelerate the manual summarization of TrojAI-related literature being curated at the [TrojAI Literature Review GitHub repository](https://github.com/usnistgov/trojai-literature). For each document, our method identifies sentences containing certain metadiscourse markers and then ranks them using this [implementation](https://pypi.org/project/lexrank/) of the [LexRank](https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html) algorithm. We then emit the top-ranked sentences as a summary of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terms of Use\n",
    "\n",
    "This software was developed at the [National Institute of Standards and Technology (NIST)](https://www.nist.gov) by employees of the Federal Government in the course of their official duties.  Pursuant to Title 17 Section 105 of the United States Code this software is not subject to copyright protection and is in the public domain.  It is an experimental system.  NIST assumes no responsibility whatsoever for its use by other parties, and makes no guarantees, expressed or implied, about its quality, reliability, or any other characteristic.  We would appreciate acknowledgement if the software is used.\n",
    "\n",
    "This software can be redistributed and/or modified freely provided that any derivative works bear some notice that they are derived from it, and any modified versions bear some notice that they have been modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Strategy\n",
    "\n",
    "We start with text extracted from PDF documents using [pdftotext](http://www.xpdfreader.com/). For each document's text, we:\n",
    "1. Segment the sentences and create a language model for the summarizer\n",
    "1. Identify the sentences which have metadiscource markers\n",
    "1. Apply LexRank to those sentences to rank them by their centrality\n",
    "1. Emit the top N sentences as a summary for that document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust location of and extension for document text files as needed\n",
    "TEXT_DIR = 'data/text/'\n",
    "TEXT_EXT = 'txt'\n",
    "\n",
    "# Metadiscourse marker sets - see MetaMarker class below\n",
    "MARKER_SETS = {0, 1}\n",
    "\n",
    "# LexRank parameters\n",
    "SUMMARY_SIZE = 7 # Number of sentences in summary\n",
    "THRESHOLD = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import regex as re\n",
    "import spacy\n",
    "import en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "from lexrank import STOPWORDS, LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dir = Path(TEXT_DIR)\n",
    "text_files = list(text_dir.glob(\"*.{}\".format(TEXT_EXT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements heuristics to identify sentences with metadiscourse markers.\n",
    "\n",
    "class MetaMarkers:\n",
    "    def __init__(self, pron = True, marker_sets = MARKER_SETS):\n",
    "        mw = [\n",
    "            # marker set 0\n",
    "            set(['paper', 'work', 'research', 'article', \n",
    "                 'study', 'publication', 'section', 'approach', \n",
    "                 'method', 'technique', 'results']),\n",
    "            \n",
    "            # marker set 1\n",
    "            set(['propose', 'present', 'exploit', 'investigate', \n",
    "                 'show', 'provide', 'explore',\n",
    "                 'focus', 'consider', 'implement', 'adopt', \n",
    "                 'examine', 'expand', 'prove', 'argue', \n",
    "                 'claim', 'suggest', 'contrast', 'summarize']),\n",
    "            \n",
    "            # marker set 2\n",
    "            set(['better', 'significant', 'first', 'second', \n",
    "                 'third', 'begin', 'finally', 'therefore', \n",
    "                 'however', 'consequently']),\n",
    "        ]\n",
    "        \n",
    "        self.marker_tags = set(['PRON'])\n",
    "        \n",
    "        self.marker_words = set()\n",
    "        try:\n",
    "            for m in marker_sets:\n",
    "                self.marker_words.update(mw[m])\n",
    "        except (IndexError, TypeError):\n",
    "            print(\"Invalid marker specifier value\", file=sys.stderr)\n",
    "            raise\n",
    "\n",
    "    def is_meta(self, sent):\n",
    "        pos = set([str(w.pos_) for w in sent])\n",
    "        tok = set([str(w.lemma_) for w in sent])\n",
    "        result = self.marker_tags.intersection(pos) or self.marker_words.intersection(tok)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segment all sentences of documents to be summarized and create a language model for the summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_pat = re.compile(\"\\s+\") # Used to normalize whitespace\n",
    "all_sents = {}             # Docs to be summarized, indexed by file name\n",
    "docs = []                  # Used to create lexRank's language model\n",
    "for text_file in text_files:\n",
    "    with open(text_file) as fin:\n",
    "        text = fin.read()\n",
    "        doc = nlp(text)\n",
    "        sents = [s for s in doc.sents]\n",
    "        all_sents[text_file] = sents\n",
    "        docs.append(str(sents))\n",
    "        \n",
    "lxr = LexRank(docs, stopwords=STOPWORDS['en']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify sentences with metadiscourse markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heur_sents = {} # Contains sentences with metadiscourse markers\n",
    "sent_order = {} # Maintains sentence appearance order\n",
    "\n",
    "mm = MetaMarkers()\n",
    "\n",
    "for f, sents in all_sents.items():\n",
    "    my_sents = []\n",
    "    for s in sents:\n",
    "        if mm.is_meta(s):\n",
    "            my_sents.append(str(s).strip())\n",
    "    heur_sents[f] = my_sents\n",
    "    sent_order[f] = {s:n for (n,s) in enumerate(my_sents)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 out of 71 documents have sentences\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: For how many of the documents do we have sentences with metadiscourse markers?\n",
    "\n",
    "num = len({f for (f,t) in heur_sents.items() if t != []})\n",
    "denom = len({f for (f,t) in heur_sents.items()})\n",
    "print(\"{} out of {} documents have sentences\".format(num, denom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply LexRank and emit top N sentences in order of appearance in original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08659362.txt: In this paper, we investigate the implication of network pruning on the resilience against poisoning attacks. ... Section II briefly reviews the basics of neural networks, poison- ing attack, and network pruning. ... Neural network pruning converts an original model to a sparse model by deleting unimportant neurons and connections after training, as shown in Fig. 2. ... In this paper, we study the impact of pruning on the resilience of neural networks against poisoning attack. ... We randomly selected 4% to 5% of the original training data for poisoning against each dataset. ... It can be seen that poisoning attack is very effective in degrading the performance of trained neural network if certain amount of training data can be manipulated. ... We have shown that pruning not only improves the resource efficiency of neural networks, but also the resilience against poisoning attack.\n",
      "\n",
      "08668758.txt: For the purpose of privacy protection against deep neural networks technologies, we propose TensorClog, an imper- ceptible poisoning attack on deep learning applications. ... An Imperceptible Poisoning Attack on Deep Neural Network Applications where Âµx denotes the mean of x, 2 ... For poisoning attack on deep learning applications, there are some pre-existing studies but none of them was design for the purpose of privacy protection. ... Reference [13] proposed a method towards poisoning attack on deep learning algorithms with back-gradient optimization technique. ... Reference [14] proposed a method to generate poisoning samples with Generative Adversarial Nets, but it focuses on generating poisoning attack effectiveness without limiting the perturbation thus resulting human noticeable attack samples. ... In addition, to prove that TensorClog attack is not just a random noise, the training loss on transfer training set with ... At last, an attack example on a real-world dataset is presented, further showing the possibility of using TensorClog\n",
      "\n",
      "12049-55640-1-PB.txt: We first examine the case that Alice has hard constraints, and all available attacks have equal effort ( ... We first examine the case of hard constraints, where Alice has strict bounds on what attack vectors are available. ... We compare Alice's optimal attack against two base- lines: Random and Greedy. ... We further note that Alice Low's attack differs con- ... In contrast, Alice High's attack results in a ^ t t t very close to the best achievable forecast for her target. ... We examine Alice (performing an optimal attack), Greedy, and Random's ef- fectiveness as a function of . ... We examined Alice's effectiveness as a function of her target and constraints, as well as of Bob's model.\n",
      "\n",
      "1206.6389.txt: Under these assumptions, we present a method that an attacker can use to construct a data point that significantly decreases the SVM's classification accuracy. ... We assume that an initial location of the attack point x(0) ... For kernels including the linear kernel, the surface of the validation error is unbounded, hence the algorithm is halted when the attack vector deviates too much from the training data; i.e., we bound the size of our attack points. ... Our gradient ascent method is then used to refine this attack un- ... The middle plots show the final attack point. ... Modifications to the initial (mislabeled) attack point performed by the proposed attack strategy, for the three considered two-class problems from the MNIST data set. ... The poisoning attack presented in this paper is the first step toward the security analysis of SVM against training data attacks.\n",
      "\n",
      "1608.08182.txt: We introduce a data poisoning attack on collaborative filtering systems. ... In this paper we provide a novel technique based on stochastic gradient Langevin dynamics optimization [10] to produce malicious users that mimic normal user behaviors in order to avoid detection, while achieving attack objectives. ... In this section we describe the data poisoning attack model considered in this paper. ... We use M RmÃn to denote the original data matrix and M Rm Ãn to denote the data matrix of all m = m malicious users. ... We use the projected gradient ascent (PGA) method for solving the optimization problem in Eq. ... To alleviate this issue, in this section we propose an alternative approach to compute data poisoning attacks such that the resulting malicious users M mimics normal users M to avoid potential detection, while still achieving reasonably large utility R(M, M) for the attacker. ... We also plot RMSE/Average ratings against malicious user percentage in Figure 2 for the nuclear norm minimization under similar settings based on a subset of 1000 users and 1700 movies (items), since\n",
      "\n",
      "1644893.1644895.txt: First we demonstrate a different type of sensitivity, namely that of data poisoning. ... Robust PCA and the new robust Laplace threshold together form a new network-wide traffic anomaly detection method, antidote, that is less sensitive to our poisoning attacks. ... we plot the FNRs against the poisoning duration for the PCA detector. ... poisoning attacks using 3 chaff methods. ... Chaff rejection rates of PCA under poisoning attacks shown in Fig. 5. ... Training Period poisoning attacks using 3 chaff methods. ... We plan to go beyond rejection of poisoning data, and study methods for identifying the responsible flow for a poisoning attack by looking at correlations among links that are rejecting chaff.\n",
      "\n",
      "1703.01340.txt: In this work, we first examine the possibility of applying traditional gradient-based method (named as the direct gradient method) to generate poisoned data against NNs by leveraging the gradient of the target model w.r.t. ... Our major contributions can be summarized as: Â· We examine traditional gradient-based poisoning attack on NN and identify the poisoned data generation rate as the bottleneck of its implementation; Â· Based on our examination, we propose a generative method to substantially speed up the poisoned data gen- eration rate with slightly degraded model attack effective- ... Generate the poisoned data xp using the given method; Step 2: ... As we shall present in Section IV, the poisoned data generated by direct gradient method can effectively degrade the accuracy of the target model. ... Very importantly, the computation cost of the poisoned data generation in direct gradient method is proportional to the dimension of the input data and the complexity of the target model. ... Figure 2 illustrates the overview of the proposed generative method for poisoned data generation. ... We propose two poisoning attack methods against NNs, including a direct gradient method and a generative method.\n",
      "\n",
      "1706.03691.txt: Are there defenses that are robust to a large class of data poisoning attacks? ... For such defenders, we can generate approximate upper bounds on the efficacy of any data poisoning attack, which hold modulo two assumptions--that the empirical train and test distribution are close together, and that the outlier removal does not significantly change the distribution of the clean (non-poisoned) ... While previous work showed successful attacks on the MNIST-1-7 (Biggio et al., 2012) and Dogfish (Koh and Liang, 2017) image datasets in the absence of any defenses, we show (Section 4) that no attack can substantially increase test error against this oracle--the 0/1-error of an SVM on either dataset ... However, they open up a new line of attack wherein the attacker chooses the poisoned data Dp to change the feasible set F. Example defenses for binary classification. ... we consider the training loss on the full (clean + poisoned) data, which upper bounds the loss on ... We plot the upper bound U computed by Algorithm 1, as well as the train and test loss induced by the corresponding attack Dp. ... We next consider attacks on text data.\n",
      "\n",
      "1708.06733.txt: In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a BadNet) that has state-of-the- art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. ... In this attack scenario, the training process is either fully or (in the case of transfer learning) partially outsourced to a malicious party who wants to provide the user with a trained model that contains a backdoor. ... [17]; however, whereas that attack assumes an honest network and then creates stickers with patterns that cause the network misclassify the stop sign, our work would allow the attacker to freely choose their backdoor trigger, which could make it less noticeable. ... We implemented multiple different attacks on these backdoored images, as described below: Â· Single target attack: the attack labels backdoored versions of digit i as digit j. ... can backdoors in the U.S. traffic signs BadNet survive transfer learning, such that the new Swedish traffic sign network also misbehaves when it sees ... Having shown in Section 5 that backdoors in pre-trained models can survive the transfer learning and cause trigger- able degradation in the performance of the new network, ... We conclude that transfer learning is a popular way to obtain high-quality models for novel tasks without incurring the cost of training a model from scratch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1708.08689.txt: (1) While this high-level formulation encompasses both evasion and poisoning attacks, in both binary and multiclass problems, in the remainder of this work we only focus on the definition of some poisoning attack scenarios. ... Although this approach allows poisoning learning algorithms more efficiently w.r.t. ... c A required by our poisoning attack, we use T = 60 iterations. ... tably, our work is also the first to show (in a more systematic way) that poisoning samples can be transferred across different learning ... In this work, we have considered the threat of training data poisoning, i.e., an attack in which the training data is purposely manipulated to maximally degrade the classification performance of learning algorithms. ... We have also empirically shown that poisoning samples designed against one learning algorithm can be rather effective also in poisoning another algorithm, highlighting an in- ... The main limitation of this work is that we have not run an extensive evaluation of poisoning attacks against deep networks, to thoroughly assess their security to poisoning.\n",
      "\n",
      "1802.03043.txt: At last, this work validates the proposed PoTrojans on two real-life deep learning models. ... the proposed PoTrojans working on two popular real-size NN learning model: AlexNet[9] and VGG16[10]. ... However, as shown in section III, we argue PoTrojans could work on regression models with the same size of AlexNet and VGG16. ... We insert the PoTrojans at every layer of both models. ... As is shown in Table III, the inserted PoTrojans are triggered in every insertion layer, i.e., the triggering rates are 100%. ... When the PoTrojans are triggered, we expect the models to output target labels. ... This paper proposes to design powerful neuron-level trojans or PoTrojans and insert them in pre-trained deep learning models.\n",
      "\n",
      "1802.07295.txt: We argue that the effectiveness of a poisoning attack drops to zero if the poisoned data is easily detected and removed, and that it is necessary to consider the detectability of an attack point as well as the theoretical optimality of the poisoned data. ... An example of an attack point generated using this outlier term is shown in Figure 3. ... An example of an attack point generated using this outlier term is shown in Figure 4. ... As in Experiment 1, two attack instances are generated for each training dataset using the modified attack method described in Section IV with the same = 0.1 however now with the attacker distance threshold parameter datt set to 1. ... At each attacker distance threshold, we calculate the outlier score, evaluated on a poisoned dataset containing the legitimate test data augmented with the appropriate attack instance. ... In this work, we have shown that the attack proposed in [21] can be easily defeated using outlier detection techniques. ... Our results show a clear correlation between the attack strength and detectability of adversarial attack instances when attacking LASSO regression augmented with novelty and outlier detection.\n",
      "\n",
      "1803.03965.txt: Thus, the goal of the proposed poisoning method is to generate the boundary pattern, which is then used to shift discriminant plane towards the central of abnormal data during model retraining. ... B. Performance of the Proposed Poisoning Method over Syn- thetic Data Sets ... To demonstrate the attacking effects of chronic poisoning, we first evaluated the performance of the proposed poisoning method against six different learning models on synthetic data sets. ... C. Performance of the Proposed Poisoning Method over real Data Sets ... To further demonstrate the effectiveness of the proposed poisoning method against different learning models, we carried out more experiments on KDDCUP99, NSL-KDD and Kyoto 2006+ data sets. ... In this paper, we have proposed a novel poisoning method by using the EPD algorithm. ... Extensive experiments on synthetic and real data sets demonstrate the effectiveness of the proposed poisoning method against different learning models and state-\n",
      "\n",
      "1804.00792.txt: Informed by these visualizations, we craft a 50 poison instance attack on a deep network which achieves success rates of up to 60% in the end-to-end training scenario. ... If, at test time, the model mistakes the target instance as being in the base class, then the poisoning attack is considered successful. ... In this case, a \"one-shot kill\" attack is possible; by adding just one poison instance to the training set (that is labeled by a reliable expert), we cause misclassification of the target with 100% success rate. ... We select both target and base instances from the test set and craft a poison instance using Algorithm 1 with maxIters = 1000. ... We then evaluate our poisoning attack by training the model with the clean data + single poison instance. ... (b) To make the attack successful, we construct 50 poison instances from 50 random base instances that are \"watermarked\" with a 30% opacity target instance. ... The clean-label targeted poison attack is similar to adversarial examples in the sense that they both are used for misclassifying a particular target instance.\n",
      "\n",
      "1805.05098.txt: So, in this paper, we propose a hardware-software collaborative attack framework to inject hidden neural network Trojans, which works as a back-door without requiring manipulating input images and is flexible for different scenarios. ... Â· We define a threat model of neural network attack. ... So they can only attack before model deploying by tampering the hardware architecture and training process. ... 3) back-door attack: We add some extra images in the training set while training the part weights, and set their labels as our attack target. ... Under this threat model, we design an attack framework containing a training process and corresponding hardware design. ... Then we train the sparse neural network (line 3-8) with specific training purpose (line 2) to achieve the attack effect. ... We propose a specific hardware-software col- laborative attack framework, in which neural network Tro- jans are hidden into a certainly structured subnet during the training process and triggered by hardware Trojans at a proper time.\n",
      "\n",
      "1805.12185.txt: Although the pruning defense is successful on all three backdoor attacks, we de- ... While fine-tuning provides some degree of protection against backdoors, we find that a combination of pruning and fine-tuning, which we refer to as fine-pruning, is the most effective in disabling backdoor attacks, in some case reducing the backdoor success to 0%. ... We design a new pruning-aware backdoor attack that, unlike prior attacks in litera- ture [18,27,10], ensures that clean and backdoor inputs activate the same neurons, thus making backdoors harder to detect. ... To evaluate the proposed defense mechanisms, we reproduced three backdoor attacks described in prior work on face [10], speech [27] and traffic sign [ ... [10] implemented a targeted backdoor attack on face recog- ... We evaluate the fine-pruning defense on all three backdoor attacks under both the baseline attacker as well as the more so- ... We cannot guarantee that our defense is the last word in DNN backdoor attacks and defenses.\n",
      "\n",
      "1806.05768.txt: Finally, we discuss the potential defenses to protect neural networks against hardware Trojan attacks. ... This work introduces, for the first time, hardware Trojan attacks in the scope of neural networks. ... In this paper, we focus on the hardware Trojan attack on neural network circuit components, while we expect the hardware Trojan targeting on the weights would yield a similar impact as the software Trojan or fault injection attack. ... Our proposed methodology provides an adversary the flexibility in selecting the targeted layer of a neural network for injecting hardware Trojan. ... Since our algorithms attempt to minimize the hardware modification, which has also been verified by our experimental results, we expect such hardware Trojan detection methods would be ineffective for defeating the proposed attack. ... On the other hand, although no prior work has stud- ied hardware Trojans on neural networks, defense strategies against adversarial examples might possibly be extended to improve the robustness of neural network models against hardware Trojan injection. ... In this work, we have introduced the new hardware Trojan attack on neural networks and expanded the taxonomy of neural network attacks.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1807.00459.txt: We show that any participant in federated learning can replace the joint model with another ... We show that these attacks are not effective against federated learning, where the attacker's model is aggregated with hundreds or thousands of benign models. ... This works in any round of federated learning but is more effective when the global model is close to convergence-- ... Because the attacker may be selected only for a single round of training, he wants the backdoor to remain in the model for as many rounds as possible after the model has been replaced. ... shows that the attack causes the next global model Gt+1 to achieve 100% backdoor accuracy when = n = 100. ... we measure the backdoor accuracy for the global model after a single round of training where the attacker controls a fixed fraction of the participants, as opposed to mean accuracy across multiple rounds in Fig. 4.(d). ... Via model averaging, federated learning enables thousands or even millions of participants, some of whom will inevitably be malicious, to have direct influence over the weights of the jointly learned model.\n",
      "\n",
      "1808.05760.txt: We show on both synthetic and real-world data that the attack is effective. ... Even if some context x cannot be strongly attacked, the attacker might be able to weakly attack it. ... In each attack trial, we draw a single target context ... Data Poisoning Attacks in Contextual Bandits 11 Figure 2 shows the attack where each panel is the historical rewards where that arm was chosen. ... In Figure 4, we show the reward poisoning on the historical data against the three target users, respectively. ... Therefore, we only show the reward poisoning on historical data restricted to the target arm (namely on ya ). ... We studied offline data poisoning attack of contextual bandits.\n",
      "\n",
      "1808.10307.txt: We consider two attack settings, with backdoor injection carried out either before model training or during model updating. ... A perturbation, used as a backdoor, is the key to the success of the proposed backdoor injection attack. ... To improve, we devise a second type of backdoor, applied through an adaptive perturbation mask, that instead takes both the data and an existing model into consideration when generating the perturbation. ... Accordingly, we hypothesize that if we can find an adaptive perturbation that can \"push\" all the data points from a given class toward the decision boundary of the target class, an attack will have high chances of success, even with a small perturbation to the original image. ... Thus, the amount of perturbation is limited: by setting a low magnitude constraint , we generate an adaptive perturbation that is small enough to be effectively learned by the victim model via data poisoning. ... In this section, we present our evaluation of backdoor injection attack for both types of backdoor perturbation. ... In contrast, the advantage of having access to the original model and training data is less striking for adaptive perturbation in both settings.\n",
      "\n",
      "1811.00636.txt: One well-studied setting for such training set attacks is data poisoning [3, 34, 25, 18, 31]. ... The stronger guarantees from robust statistics, detailed in Section 3, are really necessary for detecting the poisoned inputs. ... In this section, we show some statistics from the attacks that give motivation for why ... As an aside, we note that 5% poisoned images is not enough to capture the backdoor according to our definition in our examples from Figure 2, but 10% is sufficient. ... Our outlier detection method crucially relies on the difference in representation between the clean and poisoned examples being much larger than the difference in representations within the clean examples. ... Then, we install a backdoor of poisoned automobiles 7 Table 2: ... Furthermore, we also demonstrate that the learned representation is indeed necesary; naively utilizing robust statistics tools at the data level does not provide a means with which to remove backdoored examples.\n",
      "\n",
      "1811.03728.txt: This method analyzes the neural network activations of the training data to determine whether it has been poisoned, and, ... Our method, described more formally by Algorithm 1, uses this insight to detect poisonous data in the following way. ... method involves training a new model without the data ... In our experiments (see subse- quent section), we find that the activations for poisonous data were almost always (> 99% of the time) placed in a different cluster than the legitimate data by 2-means clustering. ... Figures 2c and 2d suggest that two clusters better describe the activations when the data is poisoned, but one cluster better describes the activations when the data is not poisoned. ... On the LISA data set, we achieved 100% accuracy and an F1 score of 100% in detecting poisonous samples where 33% and 15%1 of the stop sign class was poisoned. ... : Activations of the 7+ class, which has been poisoned with data sourced from the 4, 5, and 6 classes, shown together with activations of legitimate data from the 4, 5, and\n",
      "\n",
      "1812.00292.txt: In Section IV, we evaluate SentiNet against localized universal attacks taken from the literature, as well as against physically printed adversarial patches. ... For each attack, we measured the effectiveness of SentiNet in terms of the number of attacks detected. ... To better understand the behavior of SentiNet, we have a closer look at the distribution of the benign and adversarial images with respect to the decision boundary used by SentiNet. ... The patch and an example of adversarial input is shown in Figure 6c. ... : In this attack, we consider an adversary who intends to create an input image that tricks SentiNet to detect the adversarial object in a different region where the adversarial object is located. ... To evaluate then attack, we generate 40 adversarial patches as described by Brown at al. ... With the adversarial input detection and class proposal, SentiNet can also analyze the second or subsequent proposed classes, raising the possibility of using SentiNet to detect unsuccessful, attempted attacks.\n",
      "\n",
      "1812.00483.txt: By empirically studying four deep learning systems (including both individual and ensemble systems) used in skin cancer screening, speech recognition, face verification, and autonomous steering, we show that such attacks are (i) effective - the host systems misbehave on the targeted inputs as desired by the adversary with high probability, (ii) evasive - the malicious models function indistinguishably from their benign counterparts on non-targeted inputs, (iii) elastic - the malicious models remain effective regardless of various system design choices and tuning strategies, and (iv) easy - the adversary needs little prior knowledge about the data used for system tuning or inference. ... ing with four ML systems used in security-critical applications, we show that model-reuse attacks are effective with high probability, evasive to detection, elastic against system fine-tuning, and easy to launch. ... studies the empirical use of primitive models in the development of ML systems; Â§ 3 presents an overview of model-reuse attacks ... We first apply model-reuse attacks on individual ML systems, each integrating one feature extractor and one classifier. ... Next we show that model-reuse attacks are insensitive to system fine-tuning strategies. ... We also show that model-reuse attacks are insensitive to system fine-tuning. ... Finally, we apply model-reuse attacks on ensemble ML systems.\n",
      "\n",
      "1901.07766.txt: Then, he/she inserts a trojan into the convolutional layers of the model and gets a trigger generator corresponding to the trojan. ... she inserts the trojan into the weights of convolutional layers of the model to get a trojaned model. ... The explicit classes in the Programmable Neural Network Trojan for Pre-Trained Feature Extractor victim model could also be unknown when he/ ... Table 1 shows the original Programmable Neural Network Trojan for Pre-Trained Feature Extractor Target Model ... Third, we observe that the difficulty of trojan injection de- pends on the size of the target model. ... The way to defense trojaning attack includes two types of approach: One is to detect whether the untrusted model has a trojan or not, in order to avoid using a trojaned one. ... For all layers, the trojaned model shows almost the same distribution as the original one.\n",
      "\n",
      "1901.09247.txt: In this paper, we introduce a new type of attack motivated by adversarial machine learning, namely the over-the-air spec- trum data poisoning attack. ... The adversary performs an exploratory attack to build a classifier that can predict the outcome of transmissions, i.e., whether there will be an ACK or not if no attack. ... The difference is that in [18], the adversary performs a jamming attack during the data transmission phase to make a transmission fail while in this paper the adversary performs a spectrum data poisoning attack in the sensing phase such that the transmitter has incorrect input data to its classifier and makes the wrong decision of not transmitting. ... In particular, for the scenario studied in numerical results, only few transmission attempts are made and the achieved throughput (normalized by the optimistic throughput by an ideal algorithm to detect every idle chan- nel) drops from 98.96% to 3.13%, when the spectrum data poisoning attack is launched. ... Once a classifier is built, A uses it to predict whether there is a successful transmission (if no attack). ... For the classifier built in Section III and 500 time slots considered for transmissions under the attack, 1 busy channel is identified as idle and the transmission in this slot fails, while 3 (of 96) ... We applied adversarial machine learning (based on deep neural networks) to design an over-the-air spectrum sensing data poisoning attack that manipulates the input data of the transmitter during the run-time and fools it into making wrong transmit decisions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1902.06531.txt: Trojan attacks exploit an effective backdoor created in a DNN model by leveraging the difficulty in interpretability of the learned model to misclassify any inputs signed with the attacker's chosen trojan trigger. ... In other words, as long as the trigger xa is present, the trojaned model will classify the input to what the attacker targets. ... So if the defender is allowed to have a set of trojaned inputs as assumed in [20], [21], our STRIP appears to be able to detect class-specific trojan attacks; by carefully examining and analysing the entropy distribution of tested samples (done offline) because the entropy distribution of trojaned inputs does look different from clean inputs. ... In contrast, trojan attacks maintain prediction accuracy of clean inputs as high as a benign model, while misdirecting the input to a targeted class whenever the input contains an attacker chosen trigger. ... One advantage of this method is that the trigger can be discovered and identified during the trojaned model detection process. ... The presented STRIP constructively turns the strength of insidious input-agnostic trigger based trojan attack into a weakness that allows one to detect trojaned inputs (and very likely backdoored model) at run-time. ... Therefore, based on the 2-layer trojaned model (8-layer model has 0% FAR) produced on the CIFAR10 dataset and trigger c, we further examined those images.\n",
      "\n",
      "1902.09972.txt: Here we explore how the adversary can manipulate the model discreetly to introduce a targeted trojan trigger in a RL agent. ... In previous research on backdoor attacks on neural networks, the backdoor behavior is active only when a trigger is present in the inputs Gu et al. ... Once the agent observes the trigger, it will switch to the backdoor (adversary- ... We demonstrate the backdoor agent's behavior when a trigger is pre- sented in the environment (see Figure 4). ... We call this behavior as an unintentional trigger/activation of the backdoor policy. ... The previous discussion on unintentional triggers is related to the trojan/backdoor attack; however, the unintentional triggers (sequence of common observations) could also be considered as a category of adversarial examples for the LSTM networks, which affect/switch the long-term objective of a sequential decision-making agent. ... Specifically, we showed that a maliciously-trained LSTM network based RL agent could have reasonable performance in a normal environment, but in the presence of a trigger, the network can be made to completely switch its behavior and persist even after the trigger is removed.\n",
      "\n",
      "1902.11237.txt: In this paper we present a new backdoor attack without label poisoning Since the attack works by corrupting only samples of the target class, it has the additional advantage that it does not need to identify beforehand the class of the samples to be attacked at test time. ... Though attractive, creating such a backdoor attack is a hard task, since it is difficult to convince the network to rely on the backdoor signal to classify samples belonging to the target class. ... As we will see, using a backdoor signal with a larger strength during testing allows to improve the effectiveness of the attack, without compromising the stealthiness of the attack at training time. ... To implement the attack, the ramp backdoor signal is superimposed to a fraction = 0.3 of the digit '3' samples in D3 with strength tr = 30. ... reports the results we have got when the network is trained under a backdoor attack with target digits t1 = ... In the case of digit recognition task, we were able to successfully attack the classification networks, while keeping the backdoor signal invisible. ... Future works will then focus on a better adaptation of the backdoor signal to the classification task and the target class of the attack, with the aim of reducing the strength of the backdoor signal itself and the percentage of corrupted signals required for a successful attack.\n",
      "\n",
      "1903.00317.txt: In the remote interaction setup we consider, the proposed strategy is to identify markers of the model input space that are likely to change class if the model is attacked, allowing a user to detect a possible tampering. ... In this context, we are interested in providing a first approach to detect the tampering of a neural network-based model that is deployed on a device by the model creator. ... We stress that one can consider as an attack any action on a model (i.e., on its weights) ... In such cases where the model is only accessible through its query API, one can think about a black-box testing approach [7]: it queries the suspected model with specific inputs, and then com- ... For this approach to be practical, the inputs used for querying the model shall be chosen wisely. ... The WGHT algorithm has high performance peaks for the MLP model, with up to half of triggered mark- ers for the fine-tuning attack, and a ratio of 0.385 for flooring (i.e., more than one third of markers are triggered); it has the lowest performances of the three proposed algorithms, specifically for the IRNN model. ... We proposed algorithms that craft markers to query the model with; the challenger detects an attack on the remote model by observing prediction changes on those markers.\n",
      "\n",
      "1903.06638.txt: Recent work has identified that classification models imple- mented as neural networks are vulnerable to data-poisoning and Trojan attacks at training time. ... We present TrojDRL, the first demonstration of Trojan attacks on DRL agents. ... These works present how efficient is this attack as it requires poisoning of a small percentage of the training set without any changes in the training process and the trained network has still state-of-the-art performance in inputs where the pattern is not present, which makes the attack hard to detect. ... ate the attacks that we consider under each threat model. ... Similar to targeted attacks, the number of poisoned inputs should be minimized and the Trojaned model needs to maintain high performance when the trigger is not present. ... We now adopt the perspective of a defender that wishes to detect if a Trojan is present in a trained model, identify or reverse-engineer the trigger used by the attacker, and mitigate a known Trojaned model to produce a new model where the trigger is not effective. ... We poisoned 80K states during training, where each action was chosen 8900 times.\n",
      "\n",
      "1905.05897.txt: With such observation, the most obvious approach to forge a black-box attack is to optimize a set of poisons {x(j) ... Feature Collision Attack tends to fail in the black-box setting because it is difficult to make the poisons Transferable Clean-Label Poisoning Attacks on Neural Nets Target ... Noticing (Shafahi et al., 2018) usually use multiple poisons to attack one target, we start by deriving the neces- sary and sufficient conditions on the set of poison features {(x(j) p )} ... If not explicitly specified, we take the first 4800 images from each of the 10 classes (a total of 48000 images) in the training set to pre-train the victim models and the substitute models ((i)). ... When crafting the poisons, we use the same Dropout probability as the models were trained with. ... We use Adam (Kingma & Ba, 2014) with a relatively large learning rate of 0.04 for crafting the poisons, since the networks have been trained to have small gradients on images similar to the training set. ... In the last section, we trained the substitute models on the same training set as the victim.\n",
      "\n",
      "1905.06494.txt: The authors in (Ma et al., 2018) show that the offline attack strategy on LinUCB-type contextual bandit algorithm ( ... Note that we define the attack goal as forcing the bandit algorithm to pull arm a at the next round with high probability. ... The following result shows that there exists at least one optimal solution of problem P1, i.e., one optimal offline attack for the -greedy algorithm. ... Hence, there exists at least one optimal offline attack for the -greedy algorithm. ... Now we derive the offline attack strategy for the classical UCB algorithm. ... Hence, there exists at least one optimal offline attack for the UCB algorithm. ... As far as we know, this is the first negative result showing that there is no robust bandit algorithm that can be immune to the adaptive online attack.\n",
      "\n",
      "1905.10447.txt: In this paper, we describe a significantly more powerful variant of the backdoor attack, latent backdoors, where hidden rules can be embedded in a single \"Teacher\" model, and automatically inherited by all \"Student\" models through the transfer learning process. ... In this work, we explore the possibility of a more powerful and stealthy backdoor attack, one that can be trained into the shared \"teacher\" model, and yet survive intact in \"student\" models even after the transfer learning process. ... Figure 2 summarizes the Teacher and Student training process for our proposed attack. ... We now describe the attack model of our design. ... We evaluate the proposed latent backdoor attack via two metrics measured on the Student model: 1) attack success rate, i.e. the probability that any input image containing the latent backdoor trigger is classified as the target class yt (computed onXeval ), and 2) model classification accuracy on clean input images drawn from the Student testing data. ... This means that the proposed latent backdoor attack does not compromise the model accuracy of the Student model (on clean inputs), thus the infected Teacher model is as attractive as its clean version. ... It is also impractical under our attack model, as the defender does not have access to the poisoned training set (used by the Teacher).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1905.12457.txt: In this paper, we implement a backdoor attack against LSTM-based text classification by data poisoning. ... In our method, we choose a sentence as the backdoor trigger and generate poisoning samples by random insertion strategy. ... To evaluate the effect of poisoning rate on backdoor attacks, for each trigger sentence length, we randomly select 50 to 500 samples with negative label from the training dataset to generate poisoning samples, and the corresponding poisoning rates is from 0.5% to 5%. ... Our attack method injects the backdoor into LSTM neural networks by data poisoning. ... the trigger sentence in positions where it is semantically correct in the context so as to conceal the backdoor attack. ... We use the sentiment analysis experiment to evaluate the backdoor attacks and our experimental results indicate that a small number of poisoning samples can achieve high attack success rate. ... Our future work will focus on the defense against this backdoor attack and further study the influence of the trigger sentence content on attacks.\n",
      "\n",
      "1905.13409.txt: We specifically focus on a class of attacks known as backdoor attacks, where the adversary manipulates training data and/or the training algorithm and parameters of the model in order to embed an adversarial (classification) rule into the model [8], [11]. ... then propose a pruning algorithm that utilizes the reverse-engineered backdoor trigger to remove the backdoor adversarial rule from the model. ... This defense mechanism assumes that the backdoor adversarial rule in the model is implemented by a large change in activation for the neurons that represent the backdoor features. ... We train a model with this poisoned data, obtaining a classifier that contains the backdoor. ... We then perform our adversarial embedding attack on the baseline model. ... The defense is successful on the baseline models, and the attack success rate on the retrained model is low (0% to 1.5%), but the retrained models from the model with adversarial embedding retains the backdoor behavior, and we see an attack success rate of above 90%. ... To understand why our attack evades the defense, we analyze the latent representations of the VGG model trained on the CIFAR-10 dataset.\n",
      "\n",
      "1906.10842.txt: We consider the problem of having a set of trained models, F = {fn }N n=1 , where some of them are infected with backdoor attacks. ... ROC-curves for detection of models with backdoor attacks (i.e., poisoned models) for baseline, random input images, and our proposed ULPs with M {1, 5, 10} on MNIST (a), CIFAR10 ... tacks (i.e., poisoned models) for random input images, and our proposed ULPs with M {1, 5, 10} on GTSRB (a) and Tiny- ... We observe that ULPs are able to detect poisoned models with AUC = 0.94 for M = 5 patterns. ... Also, we trained 200 poisoned and 200 clean models with random ResNet18-like architectures ... the generalizability of the networks across different sizes of triggers on the GTSRB dataset, we trained ULPs (with M=5) on poisoned models with 7 Ã 7 triggers. ... Then, we tested the ULPs on detecting poisoned models containing 5 Ã 5 trigger attacks.\n",
      "\n",
      "1907.07296.txt: A multi-faceted visualization scheme summarizes the attack results from the perspective of the machine learning model and its corresponding training dataset, and coordinated views are designed to help users quickly identify model vulnerabilities and explore potential attack vectors. ... To demonstrate our framework, we explore model vulnerabilities to data poisoning attacks. ... To analyze the results of an attack, the framework should support overview and details-on-demand: Â· Model Overview - D2.1, summarize prediction performance for the victim model as well as the poisoned model (T2.2); Â· Data Instances - D2.2, present the labels of the original and poisoned data instances ( ... To support comparisons between the victim and poisoned model, we apply the corresponding poisoning color to the border of poisoned instances and stripe patterns to the data instances whose class prediction changed after the attack. ... By focusing on targeted data poisoning attacks, our framework enables users to examine potential weak points in the training dataset and explore the impacts of poisoning attacks on model performance. ... In order to explore vulnerabilities in data poisoning, every data instance must undergo an attack. ... : In this work, we use the data poisoning attack as the main scenario to guide the visual design and interactions in the visual analytics framework.\n",
      "\n",
      "1908.00686.txt: In the absence of the model, further research found that a backdoor can be introduced to a model by poisoning a very small portion of its training data, as few as 115 images [8]. ... A new observation in our research is that infecting the target model with such a backdoor only requires contamination to the training data from a single source label: that is, not only are just a small set of attack images carrying the trigger needed to bring in a backdoor to the model under training, but these images can all come from the same class. ... how many different classes does the adversary need to select images from so that he can inject a source-agnostic backdoor to a model by mislabeling these training samples as the target class? ... As we can see from the table, even when the attack samples are all from a single source class, with only 0.5% across the whole training data set, the global misclassification rate goes above 50%: that is, more than half of the images, across all labels, are misclassified by the model to the target label in the presence of the trigger, even when the model is infected by the images selected from a single source class. ... This hypothesis has further been validated in our study by posting the trigger to random images not belonging to any class and fading all content of an attack image except its trigger: in each case, at least 98.7% of the images were assigned the target label (6th row of Table II), even by the model infected by a small set of samples from a single source class. ... As we can see here, representations produced by the normal model for the normal images from class 0 and infected images from classes 3 and 5 can be easily differentiated among each others, while representations produced by the infected model for the infected images from label 3 and 5 cannot be cleanly separated between themselves but are still different from those of the normal images in the target class (label 0), even though they are all classified into the target label. ... Further in the presence of such a backdoor, our research shows that the representation an infected model generates for an attack image becomes almost indistinguishable from that of a legitimate image with the target label.\n",
      "\n",
      "1908.01763.txt: In this work, we propose TABOR, a new trojan detection technique. ... Using various DNN models trained on different datasets as well as the various ways to insert trojan backdoors, we show that TABOR typically has much better performance in trojan detection and trigger restoration than the state-of-the-art technique Neural Cleanse. ... trojan detection that focuses on determining, given a target model, ... To mitigate the impact of a trojan backdoor upon an infected learning model, pioneering research [30] utilizes anomaly detection to identify the data input that contains a trigger. ... For a clean model carrying no trojan backdoor, we also discover Neural Cleanse mistakenly reports the model is implanted with a trojan backdoor (represented by symbol ). ... First, as is discussed in Section Â§4, a trigger pertaining to a trojan backdoor is a special adversarial sample. ... Given a target learning model, this work shows that, even for the state-of-the-art trojan backdoor detection technique, it is still dif- ficult to accurately point out the existence of a trojan backdoor without false alarms or failure identification, particularly when the trigger size, shape, and its presentation location vary.\n",
      "\n",
      "1908.03369.txt: The robustness of our Februus method is illustrated in Fig. 7. ... Our results show that the performance of the Trojaned networks after deploying our Februus framework is identical to that from a benign DNN model, while the attack success rate from backdoor trigger reduces significantly from 100% to roundly 0%. ... An illustration of Februus on clean inputs are shown in ... Comparison between Februus and other Trojan defense methods ... while our Februus method is online, which detects the Trojan within the input in run-time. ... Our Februus method is robust regardless ... The clean inputs look benign after our Februus system, making it a complete black-box defense effectively against backdoor attacks without the assumption of pre- knowledge of backdoor network or Trojan implementation.\n",
      "\n",
      "1908.10498.txt: Here, we address the challenging post-training detection of backdoor attacks in DNN image classifiers, wherein the defender does not have access to the poisoned training set, but only to the trained classifier itself, as well as to clean (unpoisoned) examples from the classification domain. ... We propose a defense against imperceptible backdoor attacks based on perturbation optimization and novel, robust detection inference. ... Under such attacks, a relatively small number of legitimate examples (images) from one or more source classes, but containing the same embedded backdoor pattern and (mis)labeled to a target class, are added to the training set so that the classifier learns to decide to the target class when the backdoor pattern is present. ... The premise behind the proposed AD framework is that for a classifier that has been backdoor data poisoned with source class s and target class t, the required perturbation size to induce misclassification to t for most images from class s is much smaller than for class pairs that have not been backdoor-poisoned Â­ ... Detection accuracy percentage of the proposed AD framework and of the NC approach for the four groups of DNN classifiers for CIFAR-10. ... In Table 2, the detection accuracy of the proposed AD and NC for each group of classifiers is shown. ... Moreover, being aware of the \"collateral damage\" effect that backdoor patterns, when added to clean images from some classes ~ s C \\ {s, t}, may also induce high misclassification rate to the target class t [17], we can always make more conservative AD inferences and claim a successful detection when only the target class is correctly inferred.3 Allowing such conservative inference, our AD achieves perfect detection for the BD-G-S group.\n",
      "\n",
      "1909.02742.txt: The first step is trigger generation from the pre-trained model, after that, we use the generated trigger to build our poisoning training set. ... As for encoding the trigger pattern into the neural network, we conduct a retraining process. ... This measurement also shows whether the neural net- work can identify the trigger pattern we added into the input images. ... In this optimization, we adjust the value of this noise to amplify a set of neuron activations h()[I] ... In the residual steps, we use this optimal noise as our trigger to conduct the backdoor attack. ... For the two types of trigger optimizations through L2 and L0 regularization, we mount our attacks on CIFAR- 10/100 and GTSRB (Stallkamp et al. ). ... We have designed a novel backdoor attack with trigger pat- terns imperceptible to human inspection, therefore boosting the success rate of backdoor attacks in practice by making the input images inconspicuous.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1909.05193.txt: Rather than poi- soning the clean data, another neural Trojan attack proposed in ... Targeted Bit Trojan (TBT) attack is proposed where the attack is performed on the deployed DNN inference model by flipping (i.e. memory bit-0 to bit-1, or vice versa) a small amount of bits of weight parameters stored in computer main memory. ... Trojan attack after the model is deployed, which is the focus of this work. ... By observing the Attack Success Rate (ASR) column, it would be evident that certain classes are more vulnerable to targeted bit Trojan attack than others. ... In neural Trojan attack, it is common that the trigger is usually visible to human eye [9, 10]. ... Thus our attack can be implemented after the model has passed through the security checks of Trojan detection. ... Our proposed Targeted Bit Trojan attack is the first work to implement neural Trojan into the DNN model by modifying small amount of weight parameters after the model is deployed for inference.\n",
      "\n",
      "1909.13374.txt: In this section, we formally introduce the Deep k-NN de- fense as well as a set of other baseline defenses against clean-label targeted poisoning attacks. ... In this section, we evaluate the effectiveness of our Deep k-NN defense and baseline defenses against the feature col- ... In this case, so long as k is set sufficiently large, there will be more than enough target training examples to cause the poisons in their midst (in feature space) to be marked as anomalous after running Deep k-NN . ... Our replication-based balancing protocol normalizes the number of examples considered by the Deep k-NN defense in feature space. ... Our Deep k-NN based defense differs in that it identifies and filters poisoned data at training time rather than at test ... However, this tactic does work well on recent poisoning attacks that use variable, learned perturbations to Deep k-NN Defense against Clean-label Data Poisoning Attacks cause misclassification via feature collisions (Shafahi et al., 2018; Zhu et al., 2019). ... The Deep k-NN defense outperforms other data poisoning baselines and provides a strong benchmark on which to measure the efficacy of future defenses.\n",
      "\n",
      "1910.00033.txt: proposed a poisoning attack with clean-label poisoned images where the model is fooled when shown a particular set of images. ... The well-known method introduced in (Gu, Dolan-Gavitt, and Garg 2017) proposes that the attacker can develop a set of poisoned training data (pairs of images and labels) by adding the trigger to a set of images from the source category and changing their label to the target category. ... , we pro- pose a stronger and more practical attack model where the poisoned data is labeled correctly (i.e, they look like target category and are labeled as the target category), and also the secret trigger is not revealed. ... We are interested in generalizing the attack so that it works for novel source images (not seen at the time of poi- soning) and also any random location for the trigger. ... We use source and target pairs to generate poisoned images using algorithm (1). ... After adding the poisons to the training data, we train a binary image classifier to distinguish between source and target images. ... We see that before the attack, most patched source images are correctly placed on the left of the boundary, but after the attack (adding poisoned targets labeled as target to the training data), the classifier has shifted so that some of the patched sources have moved over from the left to the right side.\n",
      "\n",
      "1910.03137.txt: We instead propose Meta Neural Trojaned model Detection (MNTD), a new approach for detecting Trojaned models. ... If the Trojan attack approach is known, we could simply apply it to generate Trojaned model samples. ... Suppose we have an oracle that knows the exact attack setting (Trojan) used by the adversary, we can generate a set of shadow models without any Trojan and a set of models with the same Trojan that may appear in the target model. ... We would like to train a meta-classifier to detect whether a target model has Trojan or not, but we assume that our defender has no knowledge of the adversary's attack approach or setting. ... In Figure 4, we show some examples of the Trojan triggers generated by this approach in training shadow models on the MNIST dataset. ... The workflow is same for our one- class MNTD approach except that we do not need to train the Trojan shadow models. ... To train the jumbo set of 8 Trojaned models, we try our best to generate different Trojan patterns.\n",
      "\n",
      "1911.07399.txt: In this paper, we propose NeuronInspect, a framework to detect trojan backdoors in deep neural networks via output explanation techniques. ... While the later method is not practical because model users do not have any backdoor samples with a trigger in the validation set. ... Previous trojan backdoor detection technique, such as Neural Cleanse and TABOR (Wang et al. ; ... From the explanation heatmap, we can extract some features from it and use the outlier detection algorithm to find out the existence of a trojan backdoor trigger. ... The results of the trojan backdoor detection are shown in Table 3 Efficiency ... We point out that one solution for trojan backdoor detections in DNNs is to look into the output explanation. ... work architecture for implement trojan backdoor attack on different dataset.\n",
      "\n",
      "1911.08040.txt: one to extract a backdoor poison signal, detect poison target and base classes, and filter out poisoned from clean samples with proven guaran- tees. ... Theorem 4.1 al- lows us to extract the poison signal Âµ as the largest eigen- vector of from a set of clean and poisoned samples that are labeled as the poison target class. ... In prac- tice, we find that this approach can separate poisoned sam- ples from clean samples with high accuracy for poisoned and clean samples when using the poison base class as the loss function's cross-entropy target, as shown in results from large-sized poison scenarios in Appendix Table 14 and small-sized poison scenarios in Appendix Table 15. ... So far, we have proposed a method to detect poison sig- nal (Â§ 4) and filter poisoned samples from a particular poi- son target class (Â§ 5). ... ing poison target class from a BP poisoned dataset in all our 18 experiments, as shown in Appendix Table 10 and 12. ... Combining these insights about a poisoned clas- sifier model's `poison' neuron weights and activations with Â§ A.2, we propose a method to recover poison signals in the input layer, detect poison target class and, subsequently, poisoned images. ... In Â§ 4.2, we show how we filter these input poison signals and use them to separate poisoned from clean samples with guarantees in Â§ 5.\n",
      "\n",
      "1911.10312.txt: This work corroborates a run-time Trojan detection method exploiting STRong Intentional Perturbation of inputs, is a multi-domain Trojan detection defence across Vision, Text and Audio domains--thus termed as STRIP-ViTA. ... STRIP-ViTA is the first confirmed Trojan detection method ... Then we have applied STRIP-ViTA with developed perturbation methods to detect Trojan inputs on those models, respectively. ... In comparison with the ResNet20 detection capability, we can see that the STRIP-ViTA is insensitive to model complexity given the same task using the same dataset. ... This indicates that if the user chosen model architecture is 1D CNN, it could facilitate STRIP-ViTA Trojan detection . ... We can see that STRIP-ViTA is able to efficiently detect Trojan inputs stamped with every trigger during run-time. ... One advantage of this method is that the trigger can be discovered and identified during the Trojaned model detection process.\n",
      "\n",
      "1912.02771.txt: Standard backdoor attacks are based on randomly selecting a few natural inputs, applying the backdoor trigger to them, setting their labels to the target label, and injecting them into the training set. ... The starting point of our work is the observation that, for backdoor attacks to be successful, the poisoned inputs need to be hard to classify without relying on the backdoor trigger. ... For each target label, we evaluate attacks poisoning different fractions of the training inputs from that class--0.4%, 1.5%, 6%, 25%, and 100% (corresponding to 20, 75, 300, 1250, and 5000 inputs, respectively). ... Overall, both approaches lead to effective attacks, achieving a high attack success rate with relatively few poisoned inputs. ... Overall, we find that the attack can still be successful with relatively few poisoned inputs even when the trigger visibility is significantly reduced. ... The loss of poisoned inputs without the trigger remains high throughout training, indicating that they cannot be classified correctly by the model without relying on the backdoor trigger. ... In contrast to the original attack, the poisoned inputs from our attacks appear label consistent.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1912.06895.txt: This paper focuses on methods for the general preven- tion of potential attacks on publicly-released convolutional features, so that image data can be shared for a particular vision task without leaking sensitive or private information. ... To defend against the byproduct attack of reconstruct- ing original images from the convolutional features, we pro- pose a framework that applies a deep poisoning function to ... (x) may contain information both pertinent to image classification C and image recon- struction R, as shown in Fig.3. ... To begin, we use the ImageNet dataset [41] for the target task of image classification, and we require that the visual information within the convolutional features is decimated such that images reconstructed from poisoned features are illegible from a perceptual standpoint. ... To simulate an attack from an adversary, we use the featurizer to infer con- volutional features for images in image set ... The proposed DPF is learned based on a pre-trained image reconstructor and defends this specific reconstructor effectively as shown in Fig.6. ... In this paper, we introduce the concept of a Deep Poi- soning Function (DPF) that, when applied to convolutional features learned for a specific target vision task, enables the privacy-safe sharing of image data.\n",
      "\n",
      "2002.11497.txt: Here, we focus on two scenarios where (1) a model is trained from scratch on the training set containing multiple poisons, and (2) we update a trained model on the poisoned training set. ... In Fig- ure 3, we illustrate how this property reduces the differences in gradients during the training of a model with poisons in the context of indiscriminate attacks. ... In this section, we evaluate the effectiveness of training a model with DP optimizers as a defense against data poisoning attacks. ... We then quantify the resilience of training a model with DP optimizers against indiscriminate (Â§6.2) and targeted poisoning attacks (Â§6.3 and Â§6.4). ... For each model, we increase the intensity of our attacks by blending 0, 1, 2, 3, 4, 5, 10, 20, 30, and 40% of poisons like [41]. ... Model poisoning is out-of-scope for our work because in the poisoning attacks we consider, the adversary manipulates the training set, not the gradients. ... Hence, we monitor the accuracy of a model over the clean data and poisons during training in the random LF and the SOTA attacks formulated by [41].\n",
      "\n",
      "2003.03675.txt: Figure 1b show examples for the dynamic backdoor with different triggers for the same target label. ... We first introduce how to use our Random Backdoor technique to implement a dynamic backdoor for a single target label, then we generalize it to consider multiple target labels. ... More concretely, the adversary implements the dynamic backdoor for multiple target labels using the BaN technique as follows: ... The result of our dynamic backdoor techniques for a single target label. ... The result of our dynamic backdoor techniques for multiple target label. ... To make it clear, we train the backdoor model Mbd for all possible labels set as target labels, but we visualize the triggers for a single label to show the dynamic behaviour of our Random Backdoor technique with respect to the triggers' pattern and locations. ... They are both able to achieve almost the same accuracy of a clean model, with a 100% working backdoor, for a single target label.\n",
      "\n",
      "2003.07233.txt: Trojan attacks, also called backdoor or trapdoor attacks, involve modifying a machine learning model to respond to a specific trigger in its inputs, which, if present, will cause the model to infer an incorrect response. ... However, it is unclear how well these results generalize to other triggers, methods of embedding the trig- gers, data and model modalities, and whether detecting tro- ... Utilizing the framework above, we conduct some initial exper- iments in training classification models with backdoors, with the goal of understanding how training hyperparameters and data configuration affect the trigger embedding and model performance. ... For our architecture, we use an Actor-Critic model with a shared embedding, shown in Fig. 12, and ReLU activations. ... We also include final results for our simple RL model. ... Fig. 13 shows that models trained with datasets where the triggers that are placed in static locations have a higher triggered data accuracy while maintaining clean data per- formance. ... For classification, we were able to show the effect that batch size and static versus dynamic trigger location has on trigger embedding MNIST models, as well as compare model yield and Neural Cleanse detection performance on compared\n",
      "\n",
      "2003.08633.txt: Furthermore, we consider the detection of image-scaling attacks and derive an adaptive attack. ... Our findings show that an adversary can significantly conceal image manipulations of current backdoor attacks [5] and clean-label attacks [12] without an impact on their overall attack success rate. ... We finally mount the image-scaling attack with S as source image and T as target. ... We also show that clean-label attacks cannot be detected if our new adaptive attack is applied to the manipulated images. ... In contrast to backdoor attacks, both defenses can more reliably spot the manipulations by the clean-label attack. ... As scaling attacks are agnostic to the model and poisoning attack, other backdoor techniques are also applicable whenever a manipulation needs to be concealed. ... The detection if the whole image is changed can be circumvented by using our proposed adaptive image-scaling attack variant.\n",
      "\n",
      "2003.08837.txt: One approach to this problem is provided by the area of explainable AI (XAI, cf. subsection 4.3), which seeks to make decisions taken by an AI system comprehensible to humans and thus to mitigate an essential shortcoming of cAI systems. ... Whereas previous survey articles like the ones cited above focus on attacks and immediate countermeasures on the level of the AI system itself, our publication takes into account the whole data supply chain (cf. section 2) and the fact that the AI system is just part of a larger IT system. ... On the one hand, for doing so, we draw up a more complete list of attacks which might ultimately affect the AI system. ... On the other hand, we argue that defences should not only be implemented in the AI systems themselves. ... A lot of research has been done on how to mitigate attacks on AI systems [12, 53, 1]. ... Technical measures outside the AI system can be applied to increase IT security. ... the AI system focuses on these spuri-\n",
      "\n",
      "2004.00225.txt: We perform the same visualization in Figure 3 (middle) for a successful attack with 5 poisons using MetaPoison. ... This suggests that the poisons adopt features similar to the specific target image to such an extent that the network no longer has the capacity to class-separate the target from the poisons. ... This attack shows that the poisons need not come exclusively from one class and that it's possible to use poisoning to arbitrary control victim label assignment. ... We save the state of the poisons at every several craftsteps, fully train 20 victim models from scratch on each of those poisons, and plot the average adversarial loss on the target across those victim models (orange line). ... We craft 6 sets of poisons under the same settings (500 poison dogs, target bird with ID 5) with different random seeds and compare their victim evaluation results. ... Since there is already stochasticity in training a victim model even for the same set of poisons (see, e.g., Sec. ... , we train 300 victim models on each set of poisons and plot a histogram of the resulting adversarial loss for each in Figure 15 (top).\n",
      "\n",
      "2005.00191.txt: To mitigate such limitations, (Zhu et al., 2019) proposed the Convex Polytope attack, which crafts a set of poisons ... Bullseye Polytope is the first clean-label poisoning attack being proposed for a multi-target threat model. ... In this section, we describe the Convex Polytope attack (Zhu et al., 2019) against transfer learning. ... To craft a more generalizable attack, we consider a set of NK target images of the same object and simply compute the average of their target feature vectors, , and perform Bullseye Polytope on . ... In this section, we first evaluate Bullseye Polytope (BP), against Convex Polytope (CP) with the substitute networks and datasets used in (Zhu et al., 2019) in single-target mode. ... tary material shows the attack success rates against each Bullseye Polytope: A Scalable Clean-Label Poisoning Attack with Improved Transferability 1 ... Furthermore, by including multiple images of the same target object when crafting the poisons, Bullseye Polytope shows attack trans- ferability of 49.56% against unseen images (of the same object).\n",
      "\n",
      "2006.06841.txt: We study backdoors in the context of deep-learning for source code. ... Can we inject backdoors in deep models of source code? ... We explore two kinds of source-code triggers: (1) fixed triggers, in which all poisoned elements contain the same syntactic piece of dead code, and (2) grammatical triggers, in which each poisoned training element receives dead code sampled randomly from a probabilistic grammar. ... How effective is our technique at eliminating backdoors, and which input representations are most useful for doing so in the domain of source code? ... Installing backdoors To install the backdoors, we poison the original training set by adding data points containing the trigger in the input, and the desired target in the output. ... Between the two representation functions, our approach largely succeeded in detecting the poisoned elements across backdoors, models, and datasets. ... In future work, it would be interesting to study backdoor attacks and defenses for other source code tasks (e.g. code completion) and model architectures (e.g. GNNs), and apply our insights pertaining to the spectral signatures method in other domains such as NLP.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006.07026.txt: While the use of data from multiple users allows for improved prediction accuracy with respect to models trained separately, federated learning has been shown to be vulnerable to backdoor attacks: a member of the federation can send model updates produced using malicious training examples where the output class indicates the presence of a hidden backdoor key, rather than benign input features. ... (validation) accuracy is close to 90% (62%) and 80% (32%), respectively, as before; however, after additional meta-training by benign users, attack accuracy varies and degrades noticeably: since backdoor examples are present in these additional meta-training rounds and fine-tuning iterations with correct labels, the ability to correctly classify backdoor classes gradually improves. ... From these experiments, we observe that backdoor attacks on federated meta-learning are (1) more successful on the attack training set (especially for mini-ImageNet), since (as expected) these examples have been used by the attacker during model poisoning, (2) similarly successful when benign users use correctly-labeled backdoor images for meta-training, and (3) considerably less successful when fine-tuning also includes correctly-labeled backdoor images. ... Backdoor accuracy is above 80% for all users when backdoor classes are not present during fine-tuning (Figs. ... Benign fine-tuning of matching net- works ( = 0.001) after attacks on Omniglot (a) Backdoor examples not used by benign users ... Benign fine-tuning of matching net- works ( = 0.001, = 0.6) after attacks on Om- niglot (a) Backdoor examples not used by benign users ... This shows the effectiveness of our proposed fine-tuning of matching networks on removing effects of backdoor attacks, particularly when benign examples in backdoor classes are available during fine-tuning.\n",
      "\n",
      "2006.07757.txt: Laishram and Phoha [35] applied the seminal DBSCAN (Density-Based Spatial Clustering of Applications with Noise) method [21] to remove outliers for SVM and showed that it can successfully identify most of the poisoning data. ... We assume the original input training data (before poisoning attack) is large and dense enough in the domain ; thus the poisoning data should be the sparse outliers together with ... We let Q be the set of z poisoning data items injected by the attacker to P, and suppose each q Q has distance larger than 1 > 0 to . ... In this paper, we study two different strategies for protecting SVM against poisoning attacks. ... For the SVM, we can assume that each poisoning point has a perturbation distance at least 1 (since ... A method for protecting SVM classifier from poisoning attack. ... For the 6 data sanitization defenses, we run the SVM algorithm C-SVC on the cleaned data to\n",
      "\n",
      "2006.08131.txt: Â· We propose a new trojan attack approach by inserting TrojanNet into a target model. ... To achieve the proposed four principles, we design a new trojan attack model called TrojanNet. ... We expect trojan attack has the ability to inject more trojans into the target model. ... We follow the attack strategy proposed in BadNet [14] to inject a trojan into the target model. ... Then we insert TrojanNet into different DNNs to launch trojan attack. ... (a): Original trigger patterns for three trojan attack methods. ... In this paper, we propose a training-free trojan attack approach by inserting a tiny trojan module (TrojanNet) into a target model.\n",
      "\n",
      "2006.14768.txt: A certified defense against a poisoning attack provides a certificate for each test sample, which is a guaranteed lower bound on the magnitude of any adversarial distortion of the training set that can corrupt the test sample's classification. ... For this defense, the base classifier may be a semi-supervised learning algorithm: it can use the entire unlabeled training dataset, in addition to the labels for a partition. ... As in DPA, we train base classifiers on each partition, this time additionally using the entire unlabeled training set: fi ... In order to avoid this, we instead choose a semi-supervised training method where the unlabeled samples are used only to learn semantic features of the data, before the labeled samples are introduced: this allows us to use the unlabeled samples only once, and to then share the learned feature representations when training each base classifier. ... We can then define partitions and base classifiers for each training set (T and U) as described in the main text: PT ... This allows for each base classifier to use a very simple \"semi-supervised learning algorithm\": if the test image and the one labeled training image provided to the base classifier belong to the same cluster, then the base classifier assigns the label of the training image to the test image. ... This is partly because we used more partitions, and hence fewer average samples per partition, in the MNIST 15 Number of Median Base Partitions Certified Clean Classifier\n",
      "\n",
      "2006.16469.txt: We report on experiments showing our attack has performance that is similar to or better than the state-of-the-art attacks in terms of attack success rate and distance to the target model, while providing the advantages of provable convergence, and the efficiency benefits associated with being an online attack that can determine near-optimal poisoning points incrementally. ... Our attack comes with a provable guarantee that it converges to the target classifier, and also provides a certified lower bound on number of poisoning points needed to reach a given target model. ... Next, we present the experimental results by showing the convergence of Algorithm 1, the comparison of attack success rates to state-of-the-art poisoning attacks, and the theoretical lower bound for reaching a given target classifier and its gap to the number of poisoning points. ... Our attack steadily reduces the maximum loss difference and Euclidean distance to the target model, in contrast to the KKT attack which does not seem to converge towards the target model reliably. ... In order to show the optimality of our attack, we calculate a lower bound on the number of poisoning points needed to induce the model that is induced by the poisoning points that are found by our attack. ... Our calculated lower bound shows that there exists a relatively large gap between the number of poisoning points, especially for the induced classifier from our attack for the target model of 5% error rate, where the lower bound is only 50% of the actual number of poisoning points used. ... For the KKT attack with target model from original process, we determine the target number of poisoning points using the size of poisoning set returned from running Algorithm 1 with 0.1-closeness and target model from original process as inputs.\n",
      "\n",
      "2007.00711.txt: ConFoc, disregards this assumption as it lets models extract the content of images on their own through the healing process. ... Figure 3 illustrates our content-focus approach (ConFoc) to defend against trojan attacks. ... As the only common characteristic among these samples is their content, the final step of the healing process (step 3) is retraining the trojaned model MT with the set XR so that the model learns to focus on the content of inputs. ... For these attacks, ConFoc was tested against two compromised models provided in [11]. ... : Efficiency of ConFoc on making models focus on content at testing. ... ConFoc (with both original and transformed inputs) constantly gets high values in these two metrics, while the other methods produce values below TABLE IV: Best Healed Models After Applying ConFoc Experimental Setup Best Healed Model Attack DS Model ID ... edgeable about ConFoc who seek to mitigate the healing procedure by infecting the model with styled adversarial samples.\n",
      "\n",
      "2020-201.txt: In this paper, we survey a myriad of neural Trojan attack and defense techniques that have been proposed over the last few years. ... Defense techniques include detecting neural Trojans in the model ... Both neural Trojan attacks (i.e. to inject Trojan's malicious functionality into neural networks) and countermeasures have been widely studied. ... Other works have proposed restoring compromised neural network [21, 29, 46, 53] and reconstructing input samples to bypass neural Trojans [14, 33, 45]. ... also proposed a poisoning attack on neural networks [50]. ... These techniques can be classified into four categories: neural network verification, Trojan trigger detection, compromised neural network restoration, and Trojan bypass schemes. ... Here, the input image gets passed through the Februus system, where Trojan trigger patterns are found and removed before they are sent to the neural network itself.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backdoor-sp19.txt: In contrast, adding the same backdoor trigger causes arbitrary samples from different labels to be misclassified into the target label. ... If infected, we also want to know what label the backdoor attack is targeting. ... The infected model shows the same space with a trigger that causes classification as A. ... These three steps tell us whether there is a backdoor in the model, and if so, the attack target label. ... The mismatch between reversed trigger and original trigger becomes more obvious in two Trojan Attack models, as shown in Figure 7. ... Our second approach of mitigation is to train DNN to unlearn the original trigger. ... Such backdoor circuits would also alter model's performance when a trigger is presented.\n",
      "\n",
      "DeepInspect-IJCAI2019.txt: We propose DeepInspect, the first black-box Trojan detection solution with minimal prior knowledge of the model. ... In ad- dition to NT detection, we show that DeepInspect's trigger generator enables effective Trojan mitiga- tion by model patching. ... Figure 2 illustrates the overall framework of our proposed Trojan detection method. ... We show that DeepInspect captures such static trigger patterns in Section 4. ... We evaluate DI's performance on single-target Trojan attack in the previous section. ... Here, we consider a more advanced backdoor attack where more than one output classes are infected using the same trigger. ... Unlike the prior work that relies on a clean dataset for Trojan detection, DeepInspect is able to reconstruct potential Trojan triggers with only black-box access to the queried DNN.\n",
      "\n",
      "subpop_finance.txt: We allow the adversary to pick a subpopulation by selecting a filter function, which partitions the population into the subpopulation to impact and the remainder of the data, whose performance should not change. ... We also evaluate both FEATUREMATCH and CLUSTERMATCH on the best filter functions the attack produces. ... All we require for a subpopulation attack is a subpopulation to target (i.e., a filter function) and an algorithm for generating the poison set given the filter function. ... In Algorithm 3, we present an algorithm for subpopulation attacks in full generality. ... To validate the threat of subpopulation attacks, we run our attacks on the Adult dataset from the UCI machine learning repository. ... We proposed a novel form of poisoning attack, called subpopulation attacks. ... We propose and evaluate two subpopulation generating algorithms, called FEATUREMATCH and CLUSTERMATCH, and adapt a standard baseline attack generation algorithm to generate subpopulation attacks.\n",
      "\n",
      "Trojaning Attack on Neural Networks.txt: The proposed attack generates the trigger from the original model in a way that the trigger can induce substantial activation in some neurons inside the NN. ... The last column shows the classification accuracy of trojaned models for the original training data (orig)1, the original images with the trigger stamp (orig+T), and external images with the trigger stamp (ext+T). ... and (d+ trojan trigger), which can be considered a (partial) trojaned model. ... Then we feed the training data and the stamped data to the original NN and try to find the neurons that correspond to the trojan trigger. ... the original dataset stamped with the trojan trigger while column 8 shows the test accuracy of the trojaned model on the external dataset stamped with the trojan trigger. ... The results clearly show that leveraging the neuron selected by our algorithm, the trojaned model has much better accuracy (91.6% v.s. 47.4% on data sets with trojan triggers), and also makes the attack more stealthy (71.7% v.s. 57.3% on the original data sets). ... Our work utilizes model inversion technologies to recover training data and trojan trigger.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Applying \"Classical LexRank\"\n",
    "\n",
    "for f,s in heur_sents.items():\n",
    "    summary = lxr.get_summary(s, summary_size=SUMMARY_SIZE, threshold=THRESHOLD)\n",
    "    sorted_sents = sorted([(sent_order[f][s],s) for s in summary])\n",
    "    \n",
    "    # Cleaning up embedded newlines and other whitespace issues with the sentences that we'll keep.\n",
    "    summary_sents = [re.sub(ws_pat, ' ', str(s)) for (_,s) in sorted_sents]\n",
    "    print(\"{}: {}\\n\".format(f.name, \" ... \".join(summary_sents))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
